{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Top Respositories for Topics on GitHub\n",
    "\n",
    "TODO (intro):\n",
    "- Introduction about web scraping\n",
    "- Introduction about github and the problem statement\n",
    "- Mention the tools you're using (python, requests, beautifulSoup, Pandas, os)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the steps we'll follow\n",
    "\n",
    "\n",
    "- we're going to scrape https://github.com/topics\n",
    "- we'll get a list of topics\n",
    "- for each topics we will get topic title, page url, topic description\n",
    "- for each topic we'll get top 20 repositories\n",
    "- for each repository we'.. get the repo name , usernam, stars, repo url\n",
    "- for each topics we'll create a csv file in the following format:\n",
    "```\n",
    "Repo name, username, stars, url\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the list of topics from github\n",
    "\n",
    "Explaination:\n",
    "\n",
    "- use requests to download the page\n",
    "- use bs4 to parse and extract information\n",
    "- use os to create a directory\n",
    "- convert the information into a pandas dataframe\n",
    "- save the dataframe into the directory\n",
    "\n",
    "Let's write a get the topics, their description and their page link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics():\n",
    "    topics_url = \"https://github.com/topics\" #url for the page we want to scrape ( check it out )\n",
    "    #download the page\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code != 200: #200 response means the page responded and we got the page\n",
    "        raise Exception(\"Failed to to load page {}\".format(topics_url))\n",
    "        \n",
    "    page_contents = response.text\n",
    "    #parse the html\n",
    "    doc = BeautifulSoup(page_contents, 'html.parser')\n",
    "    \n",
    "    #get the respective information using the doc\n",
    "    topic_dict = {\n",
    "        'title': get_topic_titles(doc),\n",
    "        'description': get_topic_desc(doc),\n",
    "        'url': get_topic_link(doc),\n",
    "    }\n",
    "    #convert the dictionary into a pandas DataFrame\n",
    "    topics_df = pd.DataFrame(topic_dict)\n",
    "    \n",
    "    return topics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the helper functions for extracting the title, description, url of topics from doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class of a html element is used to specify exactly which html element to target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(doc):\n",
    "    # find the paragraph tags with the specified class name\n",
    "    select_topic_class = \"f3 lh-condensed mb-0 mt-1 Link--primary\"\n",
    "    topic_title_tags = doc.find_all('p',{'class':select_topic_class})\n",
    "    \n",
    "    topic_titles = []\n",
    "\n",
    "    for tag in topic_title_tags: #interate through each of the tags and extract the text inside ( title )\n",
    "        topic_titles.append(tag.text)\n",
    "        \n",
    "    return topic_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_desc(doc):\n",
    "    # find the paragraph tags with the specified class name\n",
    "    select_desc_class = \"f5 color-text-secondary mb-0 mt-1\"\n",
    "    topic_desc_tags = doc.find_all('p',{'class':select_desc_class})\n",
    "    \n",
    "    topic_descriptions = []\n",
    "\n",
    "    for tag in topic_desc_tags: #interate through each of the tags and extract the text inside ( description )\n",
    "        topic_descriptions.append(tag.text.strip())\n",
    "        \n",
    "    return topic_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_link(doc):\n",
    "    # find the a tags with the specified class name\n",
    "    select_link_class = \"d-flex no-underline\"\n",
    "    topic_link_tags = doc.find_all('a', {'class':select_link_class})\n",
    "    \n",
    "    topic_urls = []\n",
    "    base_url = 'https://github.com'\n",
    "    for tag in topic_link_tags: #interate through each of the tags and extract the text inside ( link )\n",
    "        topic_urls.append(base_url + tag['href'])\n",
    "    \n",
    "    return topic_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a function to get the topic page using its link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_page(topic_url):\n",
    "    # download the page\n",
    "    response = requests.get(topic_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Failed to to load page {}\".format(topic_url))\n",
    "    # parse html\n",
    "    topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    return topic_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the repository information from each of the repositories listed on a particaular topic page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_repos(topic_doc):\n",
    "    #classes of elements we want to scrape\n",
    "    repo_class = \"f3 color-text-secondary text-normal lh-condensed\"\n",
    "    star_class = 'social-count float-none'\n",
    "    \n",
    "    #find all the respective tags\n",
    "    repo_tags = topic_doc.find_all('h1', class_=repo_class)\n",
    "    star_tags = topic_doc.find_all('a', {'class': star_class})\n",
    "    \n",
    "    #create a dictonary to store the information we are about to extract\n",
    "    topic_repos_dict = {\n",
    "        'username':[],\n",
    "        'repo_name': [],\n",
    "        'stars': [],\n",
    "        'repo_url': []\n",
    "        }    \n",
    "    \n",
    "    for i in range(len(repo_tags)): #iterate through the tags\n",
    "        repo_info = get_repo_info(repo_tags[i], star_tags[i]) # call function which gives a nested list of information we want\n",
    "        \n",
    "        topic_repos_dict['username'].append(repo_info[0]) # 1st inner list element has username\n",
    "        topic_repos_dict['repo_name'].append(repo_info[1]) # 2nd inner list element has repository name\n",
    "        topic_repos_dict['stars'].append(repo_info[2])     # 3rd inner list element has stars gotten\n",
    "        topic_repos_dict['repo_url'].append(repo_info[3])  # 4th inner list element has repository url\n",
    "    \n",
    "    return pd.DataFrame(topic_repos_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the function to retrive the information from the tags that contain repository and stars infomation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(h1_tag, star_tag):\n",
    "    # 2 a tag have the usename and repository name respectively so extract them\n",
    "    a_tags = h1_tag.find_all('a')\n",
    "    base_url = 'https://github.com'\n",
    "    \n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url = base_url + a_tags[1]['href'] # a relative path is mentioned in a tag so we combine it with base url\n",
    "    \n",
    "    # the stars are in string format eg: '76k' we want to convert it to 76000\n",
    "    stars = parse_star_count(star_tag.text.strip()) # function to extract stars as type integer\n",
    "    \n",
    "    return username, repo_name, repo_url, stars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the function to extract the number of stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_star_count(stars_str):\n",
    "    stars_str = stars_str.strip()\n",
    "    if stars_str[-1] == 'k':\n",
    "        return int(float(stars_str[:-1]) * 1000)\n",
    "    return int(stars_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a function to put all of it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics_repos():\n",
    "    try:\n",
    "        dir_name = \"./top_topic_repos\" #directory store the csv files in\n",
    "        \n",
    "        if os.path.isdir('top_topic_repos'): #check if it exists\n",
    "            print('top_topic_repos directory exists')\n",
    "        else: #create directory if it doesn't exist\n",
    "            return\n",
    "            os.mkdir(dir_name)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % dir_name)\n",
    "        \n",
    "    print(\"Scraping list of topics\")\n",
    "    topics_df = scrape_topics() # function from first part\n",
    "    \n",
    "    for index, row in topics_df.iterrows(): # iterate through the topic infos\n",
    "        print(\"scraping top repositories for {}\".format(row['title']))\n",
    "        scrape_topic(row['url'], row['title'], dir_name) # helper function to call the other functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to call all the other functions for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic(topic_url, topic_name, dir_name):\n",
    "    # filename\n",
    "    floc = dir_name + '/' + topic_name + \".csv\"\n",
    "    if os.path.exists(floc): # check if it exists and don't scrape if it does\n",
    "        print(\"The file {}.csv already exists :)\".format(topic_name))\n",
    "        return\n",
    "    #get the dataframe for information of respositories of each topic\n",
    "    topic_df = get_topic_repos(get_topic_page(topic_url))\n",
    "    #create a csv file of the dataframe\n",
    "    topic_df.to_csv(floc,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_topic_repos directory exists\n",
      "Scraping list of topics\n",
      "scraping top repositories for 3D\n",
      "The file 3D.csv already exists :)\n",
      "scraping top repositories for Ajax\n",
      "The file Ajax.csv already exists :)\n",
      "scraping top repositories for Algorithm\n",
      "The file Algorithm.csv already exists :)\n",
      "scraping top repositories for Amp\n",
      "The file Amp.csv already exists :)\n",
      "scraping top repositories for Android\n",
      "The file Android.csv already exists :)\n",
      "scraping top repositories for Angular\n",
      "The file Angular.csv already exists :)\n",
      "scraping top repositories for Ansible\n",
      "The file Ansible.csv already exists :)\n",
      "scraping top repositories for API\n",
      "The file API.csv already exists :)\n",
      "scraping top repositories for Arduino\n",
      "The file Arduino.csv already exists :)\n",
      "scraping top repositories for ASP.NET\n",
      "The file ASP.NET.csv already exists :)\n",
      "scraping top repositories for Atom\n",
      "The file Atom.csv already exists :)\n",
      "scraping top repositories for Awesome Lists\n",
      "The file Awesome Lists.csv already exists :)\n",
      "scraping top repositories for Amazon Web Services\n",
      "The file Amazon Web Services.csv already exists :)\n",
      "scraping top repositories for Azure\n",
      "The file Azure.csv already exists :)\n",
      "scraping top repositories for Babel\n",
      "The file Babel.csv already exists :)\n",
      "scraping top repositories for Bash\n",
      "The file Bash.csv already exists :)\n",
      "scraping top repositories for Bitcoin\n",
      "The file Bitcoin.csv already exists :)\n",
      "scraping top repositories for Bootstrap\n",
      "The file Bootstrap.csv already exists :)\n",
      "scraping top repositories for Bot\n",
      "The file Bot.csv already exists :)\n",
      "scraping top repositories for C\n",
      "The file C.csv already exists :)\n",
      "scraping top repositories for Chrome\n",
      "The file Chrome.csv already exists :)\n",
      "scraping top repositories for Chrome extension\n",
      "The file Chrome extension.csv already exists :)\n",
      "scraping top repositories for Command line interface\n",
      "The file Command line interface.csv already exists :)\n",
      "scraping top repositories for Clojure\n",
      "The file Clojure.csv already exists :)\n",
      "scraping top repositories for Code quality\n",
      "The file Code quality.csv already exists :)\n",
      "scraping top repositories for Code review\n",
      "The file Code review.csv already exists :)\n",
      "scraping top repositories for Compiler\n",
      "The file Compiler.csv already exists :)\n",
      "scraping top repositories for Continuous integration\n",
      "The file Continuous integration.csv already exists :)\n",
      "scraping top repositories for COVID-19\n",
      "The file COVID-19.csv already exists :)\n",
      "scraping top repositories for C++\n",
      "The file C++.csv already exists :)\n"
     ]
    }
   ],
   "source": [
    "scrape_topics_repos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ideas for future\n",
    "\n",
    "- we can just add \"?page=n\" to the topics page url where n is an iterator to get the information from the rest of the pages as well\n",
    "\n",
    "- we are going to build more web scraping projects in the future... do check out our readme file for ideas and our respositories for other projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you and do give my repository a star if this was worth your time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
